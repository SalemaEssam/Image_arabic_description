{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGD30kZov8D0"
      },
      "source": [
        "# Install Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbUw1J56drt6",
        "outputId": "6a6d83e3-a003-41ae-bc69-72c505268e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gtts in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h1Tj9uod0f5",
        "outputId": "24aeb655-cfe8-411e-8bb6-03bcaf913c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qLANfaTwAbu"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ikOU2OspdU1_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import gradio as gr\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration , BlipForQuestionAnswering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjzrFsy_wC1-"
      },
      "source": [
        "# Create APP Classses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jof90lmGZXy1"
      },
      "outputs": [],
      "source": [
        "class TranslateText:\n",
        "    def __init__(self):\n",
        "        # Intialize model and tokenizer\n",
        "        self.model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "        self.tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "    def get_translation(self,text,ar_en=False,en_ar=False):\n",
        "        if ar_en:\n",
        "            # arabic text\n",
        "            ar_text = text\n",
        "            # translate Arabic to English\n",
        "            self.tokenizer.src_lang = \"ar_AR\"\n",
        "            encoded_ar = self.tokenizer(ar_text, return_tensors=\"pt\")\n",
        "            generated_tokens = self.model.generate(\n",
        "                **encoded_ar,\n",
        "                forced_bos_token_id= self.tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "            )\n",
        "            # return translation\n",
        "            return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        else:\n",
        "            # english text\n",
        "            en_text = text\n",
        "            # translate english to arabic\n",
        "            self.tokenizer.src_lang = \"en_XX\"\n",
        "            encoded_ar = self.tokenizer(en_text, return_tensors=\"pt\")\n",
        "            generated_tokens = self.model.generate(\n",
        "                **encoded_ar,\n",
        "                forced_bos_token_id= self.tokenizer.lang_code_to_id[\"ar_AR\"]\n",
        "            )\n",
        "            # return translation\n",
        "            return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d_hscaiOdU2B"
      },
      "outputs": [],
      "source": [
        "class ImageCaption:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the models and processors for image captioning and VQA\"\"\"\n",
        "        # Load caption model\n",
        "        self.caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "        self.caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "        # Load VQA model\n",
        "        self.vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        self.vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "    def get_caption(self, image):\n",
        "        \"\"\"Generate a caption for the input image\"\"\"\n",
        "        text = \"a photo of\"\n",
        "        inputs = self.caption_processor(image, text, return_tensors=\"pt\")\n",
        "        caption = self.caption_model.generate(**inputs)\n",
        "        # Fix: Correcting processor variable reference\n",
        "        caption = self.caption_processor.decode(caption[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "\n",
        "    def ask_question(self, image, question):\n",
        "        \"\"\"Answer a question related to the input image\"\"\"\n",
        "        inputs = self.vqa_processor(image, question, return_tensors=\"pt\")\n",
        "        # Fix: Use proper method for VQA (not generate)\n",
        "        output = self.vqa_model.generate(**inputs)\n",
        "        # Decoding answer using processor\n",
        "        ans = self.vqa_processor.decode(output[0], skip_special_tokens=True)\n",
        "        return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6lv4F_oTdU2B"
      },
      "outputs": [],
      "source": [
        "class TextToAudio:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_audio(self,text,output_file_name=\"output.mp3\"):\n",
        "        try:\n",
        "            # Generate the speech using gTTS\n",
        "            tts = gTTS(text=text, lang='ar')\n",
        "\n",
        "            # Save the audio to an mp3 file\n",
        "            tts.save(output_file_name)\n",
        "\n",
        "            # Return the file path\n",
        "            return output_file_name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating audio: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D0seo6f_Yczt"
      },
      "outputs": [],
      "source": [
        "class QuestionGenerator:\n",
        "    def __init__(self):\n",
        "        # Set seed to 0\n",
        "        torch.random.manual_seed(0)\n",
        "        # Load model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3.5-mini-instruct\",\n",
        "            device_map=\"cuda\",\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        # Load Tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "\n",
        "    def suggest_questions(self,caption):\n",
        "        # Define system prompt and pass image caption to generate questions based on it\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a model designed to generate thoughtful and precise questions based on image captions. Your task is to generate two questions related to the image described by the caption.The questions should ask about very simple, specific details that a question-answering model can respond to in one or two words. Keep these two questions short and straightforward to avoid confusion.Output only the questions.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Here is an image caption: 'A person riding a red bicycle in a busy city street.'\"},\n",
        "            {\"role\": \"assistant\", \"content\": 'What color is the bicycle? \\n Is the person wearing a helmet?\\n'\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"Here is an image caption: 'A dog playing in a garden with a ball.'\"},\n",
        "            {\"role\": \"assistant\", \"content\": 'What color is the dog?\\n Are there any trees in the garden?\\n'\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": f\"Here is an image caption: '{caption}'\"},\n",
        "        ]\n",
        "        # define pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model= self.model,\n",
        "            tokenizer= self.tokenizer,\n",
        "        )\n",
        "        # define generation arguments\n",
        "        generation_args = {\n",
        "            \"max_new_tokens\": 500,\n",
        "            \"return_full_text\": False,\n",
        "            \"temperature\": 0.0,\n",
        "            \"do_sample\": False,\n",
        "        }\n",
        "        # Generate the output\n",
        "        output = pipe(messages, **generation_args)\n",
        "        # return the generated questions\n",
        "        return output[0]['generated_text'].split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yaYiTPiwRCB"
      },
      "source": [
        "# Create APP Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RNRNxdkGdU2B"
      },
      "outputs": [],
      "source": [
        "class APP:\n",
        "    def __init__(self):\n",
        "        # Initialize all the required components\n",
        "        self.translator = TranslateText()\n",
        "        self.caption_model = ImageCaption()\n",
        "        self.question_generator = QuestionGenerator()\n",
        "        self.tts = TextToAudio()\n",
        "\n",
        "    def generate_caption(self, image):\n",
        "        # Step 1: Generate image caption\n",
        "        caption = self.caption_model.get_caption(image)\n",
        "\n",
        "        # Step 2: Translate caption to Arabic\n",
        "        translated_caption = self.translator.get_translation(caption, en_ar=True)\n",
        "\n",
        "        # Step 3: Generate Arabic speech from caption\n",
        "        speech = self.tts.get_audio(translated_caption[0])\n",
        "\n",
        "        # Step 4 : Generate questions from caption\n",
        "        suggested_questions = self.question_generator.suggest_questions(caption)\n",
        "\n",
        "        # Step 5: Translate suggested questions to Arabic\n",
        "        translated_questions = []\n",
        "        for question in suggested_questions:\n",
        "            translated_questions.append(self.translator.get_translation(question, en_ar=True))\n",
        "\n",
        "        return translated_caption[0], speech , translated_questions\n",
        "\n",
        "    def answer_question(self, image, question):\n",
        "        # Translate queston from Arabic to English\n",
        "        question = self.translator.get_translation(question, ar_en=True)\n",
        "\n",
        "        # Step 4: Answer the question based on the image\n",
        "        answer = self.caption_model.ask_question(image, question)\n",
        "\n",
        "        # Step 5: Translate the answer to Arabic\n",
        "        translated_answer = self.translator.get_translation(answer, en_ar=True)\n",
        "        return translated_answer[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rI2r0QFbHaL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "cd2a8c40da55459b94176ee0ba87de15",
            "4499b1dcfe2244b5a8f062624fa1e180",
            "e7a4b7686e614495a1e258b2dd8d7ca2",
            "f3ecbf3ef7f04372b0dd7849c11c55cf",
            "796f6d09ef1d4d23be9a9426e84abc33",
            "b431c69b14a24831966e1c8669ff3ec5",
            "58582ed52c104784a8a3eae2b4199d59",
            "beb9cf0749f5451c8ab470d235ef6c3b",
            "e2b8afd10ec34c2e9f82066e2dd419a1",
            "c6af411a8825404ea60d4855e200377c",
            "e871814558144316a34788af5319279e"
          ]
        },
        "id": "O5nLqiUbdU2B",
        "outputId": "5955504e-b4d8-417c-a2f0-5a68203c6342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd2a8c40da55459b94176ee0ba87de15"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Instantiate the app\n",
        "app = APP()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF99YY3xwi-Y"
      },
      "source": [
        "# Create Gradio interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "fHBeoNpmdn5T",
        "outputId": "95fc4f32-7dd0-41ec-8203-1b907f862c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e747f3ba9980b4fb49.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e747f3ba9980b4fb49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to generate the caption, speech, and questions\n",
        "def generate_caption(image):\n",
        "    try:\n",
        "        # Get the translated caption, speech, and suggested questions\n",
        "        translated_caption, speech, suggested_questions = app.generate_caption(image)\n",
        "\n",
        "        # Clean and format questions\n",
        "        questions_cleaned = [q[0] for q in suggested_questions]  # Extract each question from the list of lists\n",
        "\n",
        "        # Ensure there are 4 questions, if less, fill with empty strings\n",
        "        while len(questions_cleaned) < 4:\n",
        "            questions_cleaned.append(\"\")  # Fill with empty strings if fewer than 4 questions\n",
        "\n",
        "        # Return caption text, audio file, and each question separately\n",
        "        return translated_caption, speech, questions_cleaned[0], questions_cleaned[1], questions_cleaned[2], questions_cleaned[3]\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return default values in case of error\n",
        "        return \"Error generating caption.\", None, \"Error: Couldn't generate question 1\", \"Error: Couldn't generate question 2\", \"Error: Couldn't generate question 3\", \"Error: Couldn't generate question 4\"\n",
        "\n",
        "# Function to answer the user question\n",
        "def answer_question(image, user_question):\n",
        "    try:\n",
        "        answer = app.answer_question(image, user_question)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Function to populate the user question textbox\n",
        "def set_user_question(question):\n",
        "    return question\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h1 align='center'>Image Captioning and Question Answering App (Arabic)</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(label=\"Upload an Image\", type=\"pil\")\n",
        "\n",
        "    with gr.Row():\n",
        "        generate_btn = gr.Button(\"Generate Caption\")\n",
        "\n",
        "    with gr.Row():\n",
        "        caption_output = gr.Textbox(label=\"Caption\", lines=2)\n",
        "        audio_output = gr.Audio(label=\"Caption Audio\")\n",
        "\n",
        "    # Create buttons for each question\n",
        "    with gr.Row():\n",
        "        question_btn1 = gr.Button(\"Suggested Question 1\")\n",
        "        question_output1 = gr.Textbox(label=\"Suggested Question\", lines=1)\n",
        "        question_btn2 = gr.Button(\"Suggested Question 2\")\n",
        "        question_output2 = gr.Textbox(label=\"Suggested Question\", lines=1)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        user_question = gr.Textbox(label=\"Ask a Question (in Arabic)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        answer_btn = gr.Button(\"Get Answer\")\n",
        "\n",
        "    with gr.Row():\n",
        "        answer_output = gr.Textbox(label=\"Answer (Arabic)\", lines=2)\n",
        "\n",
        "    # Function bindings\n",
        "    generate_btn.click(\n",
        "        generate_caption,\n",
        "        inputs=[image_input],\n",
        "        outputs=[caption_output, audio_output, question_output1, question_output2]\n",
        "    )\n",
        "\n",
        "    # Set user question when a button is clicked\n",
        "    question_btn1.click(set_user_question, inputs=question_output1, outputs=user_question)\n",
        "    question_btn2.click(set_user_question, inputs=question_output2, outputs=user_question)\n",
        "\n",
        "\n",
        "    answer_btn.click(answer_question, inputs=[image_input, user_question], outputs=[answer_output])\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PnJmkCFE1f7e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30776,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd2a8c40da55459b94176ee0ba87de15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4499b1dcfe2244b5a8f062624fa1e180",
              "IPY_MODEL_e7a4b7686e614495a1e258b2dd8d7ca2",
              "IPY_MODEL_f3ecbf3ef7f04372b0dd7849c11c55cf"
            ],
            "layout": "IPY_MODEL_796f6d09ef1d4d23be9a9426e84abc33"
          }
        },
        "4499b1dcfe2244b5a8f062624fa1e180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b431c69b14a24831966e1c8669ff3ec5",
            "placeholder": "​",
            "style": "IPY_MODEL_58582ed52c104784a8a3eae2b4199d59",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e7a4b7686e614495a1e258b2dd8d7ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb9cf0749f5451c8ab470d235ef6c3b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2b8afd10ec34c2e9f82066e2dd419a1",
            "value": 2
          }
        },
        "f3ecbf3ef7f04372b0dd7849c11c55cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6af411a8825404ea60d4855e200377c",
            "placeholder": "​",
            "style": "IPY_MODEL_e871814558144316a34788af5319279e",
            "value": " 2/2 [00:35&lt;00:00, 16.95s/it]"
          }
        },
        "796f6d09ef1d4d23be9a9426e84abc33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b431c69b14a24831966e1c8669ff3ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58582ed52c104784a8a3eae2b4199d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb9cf0749f5451c8ab470d235ef6c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b8afd10ec34c2e9f82066e2dd419a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6af411a8825404ea60d4855e200377c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e871814558144316a34788af5319279e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
